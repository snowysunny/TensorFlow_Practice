{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# chapter 3\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "#     在计算图g1中定义变量\"v\"，并设置初始值为0\n",
    "    v = tf.get_variable(\"v\", initializer=tf.zeros_initializer, shape=[1])\n",
    "\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "#     在计算图g2中定义变量\"v\"，并设置初始值为1\n",
    "    v = tf.get_variable(\"v\", initializer=tf.ones_initializer, shape=[1])\n",
    "\n",
    "# 在计算图g1中读取变量\"v\"的取值\n",
    "with tf.Session(graph=g1) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "#         在计算图g1中，变量\"v\"的取值应该为0，所以下面这行会输出[0.]\n",
    "        print(sess.run(tf.get_variable(\"v\")))\n",
    "\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "#         在计算图g2中，变量\"v\"的取值应该为1，所以下面这行输出[1.]\n",
    "        print(sess.run(tf.get_variable(\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5.]\n",
      "[3. 5.]\n",
      "Tensor(\"add_18:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([1.0, 2.0], name = 'a')\n",
    "b = tf.constant([2.0, 3.0], name = 'b')\n",
    "\n",
    "result0 = a + b\n",
    "\n",
    "# print(result)\n",
    "with tf.Session() as sess:\n",
    "    print(result0.eval())\n",
    "    tf.global_variables_initializer().run()\n",
    "    resu = sess.run(result0)\n",
    "    print(resu)\n",
    "    \n",
    "result1 = tf.add(a, b, name=\"add\")\n",
    "\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.957578]]\n"
     ]
    }
   ],
   "source": [
    "# 实现前向传播算法\n",
    "import tensorflow as tf\n",
    "\n",
    "# 声明w1、w2两个变量，这里还通过seed参数设定了随机种子\n",
    "# 这样可以保证每次运行得到的结果是一样的\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev = 1, seed = 1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev = 1, seed = 1))\n",
    "\n",
    "# 暂时将输入的特征向量定义为一个常量。注意这里x是一个1*2的矩阵\n",
    "x = tf.constant([[0.7, 0.9]])\n",
    "\n",
    "# 获得前向传播算法神经网络的输出\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "#     sess.run(w1.initializer)\n",
    "#     sess.run(w2.initializer)\n",
    "    print(sess.run(y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.957578]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev = 1, seed = 1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev = 1, seed = 1))\n",
    "\n",
    "# 定义placeholder作为存放输入数据的地方。这里维度也不一定要定义\n",
    "# 但如果维度是确定的，那么给出维度可以降低出错的概率\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(1, 2), name = \"input\")\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(y, feed_dict={x: [[0.7, 0.9]]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.957578 ]\n",
      " [1.1537654]\n",
      " [3.1674924]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev = 1, seed = 1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev = 1, seed = 1))\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = (3, 2), name = \"input\")\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(y, feed_dict={x: [[0.7, 0.9], [0.1, 0.4], [0.5, 0.8]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "After 0 training step(s), cross entropy on all data is 0.0674925\n",
      "After 1000 training step(s), cross entropy on all data is 0.0163385\n",
      "After 2000 training step(s), cross entropy on all data is 0.00907547\n",
      "After 3000 training step(s), cross entropy on all data is 0.00714436\n",
      "After 4000 training step(s), cross entropy on all data is 0.00578471\n",
      "[[-1.9618275  2.582354   1.6820377]\n",
      " [-3.4681718  1.0698231  2.11789  ]]\n",
      "[[-1.824715 ]\n",
      " [ 2.6854665]\n",
      " [ 1.418195 ]]\n"
     ]
    }
   ],
   "source": [
    "# 训练神经网络解决二分类问题\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# 定义训练数据的batch的大小，分批处理，批处理的大小\n",
    "batch_size = 8\n",
    "\n",
    "# 定义神经网络的参数\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev = 1, seed = 1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev = 1, seed = 1))\n",
    "\n",
    "#在shape的一个维度上使用None可以方便使用不大的batch大小。在训练时需要把数据分成比较小的batch\n",
    "#但是在测试时，可以一次性使用全部的数据。当数据集比较小时这样比较方便测试，但数据集比较大时，将大量数据放入一个batch可能会导致内存溢出\n",
    "x = tf.placeholder(tf.float32, shape = (None, 2), name = \"input\")\n",
    "y_ = tf.placeholder(tf.float32, shape = (None, 1), name = \"input\")\n",
    "\n",
    "# 定义神经网络前向传播的过程\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 定义损失函数和反向传播的算法\n",
    "cross_entroy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entroy)\n",
    "\n",
    "# 通过随机数生成一个模拟数据集\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "\n",
    "# 定义规则来给出样本的标签。在这里所有x1+x2<1的样例都被认为是正样本（比如零件合格）\n",
    "# 而其他为负样本（比如零件不合格）。和TensorFlow游乐场中的表示法不大一样的地方是在这里\n",
    "# 使用0来表示负样本，1表示正样本。大部分解决分类问题的神经网络都会采用0和1的表示方法\n",
    "\n",
    "Y = [[int(x1 + x2 < 1)] for (x1, x2) in X] \n",
    "\n",
    "# 创建一个会话来运行TensorFlow程序\n",
    "with tf.Session() as sess:\n",
    "#     初始化变量\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "    \n",
    "    '''\n",
    "    在训练之前神经网络参数的值：\n",
    "    w1 = [[-0.81131822, 1.48459876, 0.06532937]\n",
    "          [-2.44270396, 0.0992484, 0.59122431]]\n",
    "    w2 = [[-0.811318221], [1.48459876], [0.06532937]]\n",
    "    '''\n",
    "    \n",
    "#     设定训练的轮数\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "#         每次选取batch_size个样本进行训练\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start + batch_size, dataset_size)\n",
    "        \n",
    "#         通过选取的样本训练神经网络并更新参数\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "#             每隔一段时间计算在所有数据上的交叉熵并输出\n",
    "            total_cross_entropy = sess.run(cross_entroy, feed_dict={x: X, y_:Y})\n",
    "            print(\"After %d training step(s), cross entropy on all data is %g\" %(i, total_cross_entropy))\n",
    "        \"\"\"\n",
    "        通过这个结果可以发现随着训练的进行，交叉熵是逐渐变小的。交叉熵越小说明预测的结果和真是的结果差距越小\n",
    "        \"\"\"\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "    \"\"\"\n",
    "     可以发现这两个参数的取值已经发生了变化，这个变化就是训练的结果。\n",
    "    它使得这个神经网络能更好的拟合提供的训练数据。\n",
    "    \"\"\"\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
